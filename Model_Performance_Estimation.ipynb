{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzzS7h0yuJ38MhbWFG4gbp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Malachyiii/Model-Performance-Estimation/blob/main/Model_Performance_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n",
        "\n",
        "## ***Hey did you change your runtime to GPU?***\n",
        "\n",
        "\n",
        "1.   Select \"Runtime\" menu from the toolbar\n",
        "2.   Change Runtime Type\n",
        "3.   Hardware Accelerator = GPU\n",
        "\n",
        "\n",
        "### Purpose Statement\n",
        "The goal of this notebook is to explore Model Performance Estimation using the [NannyML](https://nannyml.readthedocs.io/en/main/how_it_works/performance_estimation.html#direct-loss-estimation-dle) package.\n"
      ],
      "metadata": {
        "id": "mboRKY38mXFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background\n",
        "\n",
        "We are not always lucky enough to have continuous ground truth for our trained models. The question therefore becomes how can we tell if our model is still performing well long after training is complete?\n",
        "\n",
        "Direct loss estimation is one possible answer. It's fairly simple in concept, and this notebook is meant to show a fairly simple example using a toy dataset. It is a 3 step process\n",
        "\n",
        "\n",
        "1.   Train Your Model (the child model)\n",
        "2.   Train a second model (The nanny model) using the loss from the first model as a target, and the child model features and predictions as features\n",
        "3.   Once your model is in production, use the nanny model to estimate the loss of the production model\n",
        "\n",
        "In this notebook we will train a simple Neural Network on our dataset, then use NannyML to do Direct Loss Estimation on the model for an unseen test set. We can then compare what NannyML thought the loss would be to the actual loss (since we have the true values for test)"
      ],
      "metadata": {
        "id": "pugTiqvWEz3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package Installation\n",
        "\n",
        "First we have to install the Nanny ML package, then we need to upgrade some of the packages that are installed natively in Colab"
      ],
      "metadata": {
        "id": "ehO7xL9xDyEn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xGP7jHwyuWjX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install nannyml scikeras\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install -U matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# !! AFTER RUNNING THIS YOU MUST RESTART !!"
      ],
      "metadata": {
        "id": "Sol2hH_sXuZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set Up"
      ],
      "metadata": {
        "id": "0hWSf0r1x4Yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nannyml as nml\n",
        "\n",
        "#Importing basic handling functions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import math\n",
        "\n",
        "#Specialty data wrangling functions\n",
        "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
        "from pandas.core.generic import is_number\n",
        "\n",
        "#importing oure necessary tensorflow functions\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.models import Model\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "BSC_3sKGuppU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We create our model args here\n",
        "class args:\n",
        "  #Overall training arguments\n",
        "  batch_size = 32\n",
        "  epochs = 500\n",
        "\n",
        "  #Model arguments\n",
        "  activation = 'relu'\n",
        "  dropout = 0.1\n",
        "  optimizer = 'adam'\n",
        "  loss = losses.MeanAbsoluteError() #loss = mean(abs((y_true - y_pred))\n",
        "\n",
        "  #Creating a callback function\n",
        "  callback = tf.keras.callbacks.EarlyStopping(\n",
        "                                  monitor='val_loss',\n",
        "                                  min_delta=0,\n",
        "                                  patience=20,\n",
        "                                  verbose=0,\n",
        "                                  mode='min',\n",
        "                                  baseline=None,\n",
        "                                  restore_best_weights=True\n",
        "                                  )"
      ],
      "metadata": {
        "id": "wOiyBkOtvh7_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test if Tensorflow is using the appropriate chip\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found')\n",
        "else:\n",
        "  print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "9rczImhokukF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f65f529-f2ab-4f1b-8168-5d01bfcde2f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Import and Cleaning"
      ],
      "metadata": {
        "id": "mry7UL4Fx0b-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read in our toy data set\n",
        "traindf, testdf, test_targets = nml.datasets.load_synthetic_car_price_dataset()\n",
        "\n",
        "#It comes with some prediction columns, but we'll drop them so we can make our own\n",
        "traindf = traindf.drop(\"y_pred\", axis = 1)\n",
        "testdf = testdf.drop(\"y_pred\", axis = 1)\n",
        "\n",
        "#Lets do a quick inspection\n",
        "print(\"The shape of the training set is: \", traindf.shape)\n",
        "print()\n",
        "print(traindf.head())\n",
        "print()\n",
        "print(\"The shape of the test set is: \", testdf.shape)\n",
        "print()\n",
        "print(testdf.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vW6jmqqwZX-",
        "outputId": "13caeacf-02d9-456f-f0b7-19ab516fa6db"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the training set is:  (60000, 9)\n",
            "\n",
            "   car_age  km_driven  price_new  accident_count  door_count      fuel  \\\n",
            "0     15.0   144020.0    42810.0             4.0         3.0    diesel   \n",
            "1     12.0    57078.0    31835.0             3.0         3.0  electric   \n",
            "2      2.0    76288.0    31851.0             3.0         5.0    diesel   \n",
            "3      7.0    97593.0    29288.0             2.0         3.0  electric   \n",
            "4     13.0     9985.0    41350.0             1.0         5.0    diesel   \n",
            "\n",
            "  transmission  y_true                timestamp  \n",
            "0    automatic   569.0  2017-01-24 08:00:00.000  \n",
            "1    automatic  4277.0  2017-01-24 08:00:33.600  \n",
            "2    automatic  7011.0  2017-01-24 08:01:07.200  \n",
            "3       manual  5576.0  2017-01-24 08:01:40.800  \n",
            "4    automatic  6456.0  2017-01-24 08:02:14.400  \n",
            "\n",
            "The shape of the test set is:  (60000, 8)\n",
            "\n",
            "   car_age  km_driven  price_new  accident_count  door_count      fuel  \\\n",
            "0      9.0    96276.0    36603.0             4.0         4.0       gas   \n",
            "1     12.0    25303.0    22210.0             2.0         5.0       gas   \n",
            "2     11.0   143756.0    23364.0             1.0         4.0       gas   \n",
            "3     10.0    88496.0    37141.0             5.0         3.0  electric   \n",
            "4     18.0   110028.0    43042.0             1.0         4.0       gas   \n",
            "\n",
            "  transmission                timestamp  \n",
            "0       manual  2017-02-16 16:00:00.000  \n",
            "1       manual  2017-02-16 16:00:33.600  \n",
            "2    automatic  2017-02-16 16:01:07.200  \n",
            "3       manual  2017-02-16 16:01:40.800  \n",
            "4       manual  2017-02-16 16:02:14.400  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning"
      ],
      "metadata": {
        "id": "6tunE15sGeMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we are just gonna drop all the date columns, as we're not doing a time series model\n",
        "train = traindf.drop(\"timestamp\", axis = 1)\n",
        "test = testdf.drop(\"timestamp\", axis = 1)\n",
        "\n",
        "#next we need to clean up our string columns\n",
        "if (train.dtypes == 'object').any():\n",
        "\n",
        "  #We will create a one-hot encoder that drops a column only if it encounters binary (ex: Male/Female) data\n",
        "  #Any columns must represent at least 1% of the data with a max of 20 categories\n",
        "  enc = OneHotEncoder(drop = 'if_binary', min_frequency=0.01, max_categories=20)\n",
        "\n",
        "  #Now we one-hot encode the data, extract the groups, add the encoded data to the data frame, and drop the string columns\n",
        "  encoded = pd.DataFrame(enc.fit_transform(train.select_dtypes(include='object')).toarray(), columns = list(enc.get_feature_names_out()))\n",
        "  train = train.select_dtypes(exclude='object')\n",
        "\n",
        "  train = pd.concat([train, encoded], axis = 1)\n",
        "\n",
        "  #Repeat for test\n",
        "  encoded = pd.DataFrame(enc.transform(test.select_dtypes(include='object')).toarray(), columns = list(enc.get_feature_names_out()))\n",
        "  test = test.select_dtypes(exclude='object')\n",
        "\n",
        "  test = pd.concat([test, encoded], axis = 1)"
      ],
      "metadata": {
        "id": "hVNfg3hywlom"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#put the target column last and make it a float\n",
        "train = np.asarray(pd.concat([train.drop(\"y_true\", axis = 1),train[\"y_true\"]], axis = 1)).astype('float32')\n",
        "\n",
        "test = np.asarray(pd.concat([test, test_targets], axis = 1)).astype('float32')"
      ],
      "metadata": {
        "id": "RHyxup_R1DzK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we create a normalization layer to standardize the data\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(train)\n",
        "\n",
        "#The decoder allows us to change the predictions back into person_count at the end\n",
        "decoder = tf.keras.layers.Normalization(axis=None, invert=True)\n",
        "decoder.adapt(train[:, -1])"
      ],
      "metadata": {
        "id": "vOAmUPv0yE7-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we normalize each of our data sets and split the Target from the Data\n",
        "train = normalizer(train).numpy()\n",
        "trainx = train[:, :-1]\n",
        "trainy = train[:, -1]\n",
        "\n",
        "test = normalizer(test).numpy()\n",
        "testx = test[:, :-1]\n",
        "testy = test[:, -1]\n"
      ],
      "metadata": {
        "id": "aZEtuomb_lN4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we fill the na values with -1\n",
        "trainx = np.nan_to_num(trainx, copy=True, nan=-1.0, posinf=-1.0, neginf=-1.0)\n",
        "testx = np.nan_to_num(testx, copy=True, nan=-1.0, posinf=-1.0, neginf=-1.0)"
      ],
      "metadata": {
        "id": "ePWeFy_O-0VG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation and Fitting"
      ],
      "metadata": {
        "id": "RUKjK_BI2JIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we get the number of columns from the training data\n",
        "size = trainx.shape[1]\n",
        "\n",
        "#Because we are using the KerasRegressor wrapper function, we must define the model and compile it in a single function\n",
        "def base_model():\n",
        "  model = tf.keras.Sequential([\n",
        "    layers.Input(size),\n",
        "    \n",
        "    #We will define three different layers of neurons, each with a dropout layer\n",
        "    layers.Dense(units = 2 * size, activation = args.activation),\n",
        "    layers.Dropout(args.dropout),\n",
        "    \n",
        "    #Block 2, half the size of block 1\n",
        "    layers.Dense(units = size, activation = args.activation),\n",
        "    layers.Dropout(args.dropout),\n",
        "    \n",
        "    #Block 3, half the size of block 2\n",
        "    layers.Dense(units = round(0.5 * size), activation = args.activation),\n",
        "    layers.Dropout(args.dropout),\n",
        "    \n",
        "    #Output lay is a single unit with no activation because we are expecting to\n",
        "    #produce a regression\n",
        "    layers.Dense(units = 1)\n",
        "  ])\n",
        "  #Printing so we can review for diagnostic purposes\n",
        "  print(model.summary())\n",
        "\n",
        "  #compile the model with the optimizer and loss, then return it\n",
        "  model.compile(optimizer= args.optimizer, loss= args.loss)\n",
        "  return model"
      ],
      "metadata": {
        "id": "fgRrs3dy2IMs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we instantiate our model. We are using the KerasRegressor Wrapper in this\n",
        "#notebook, Normally all of these parameters would be in .fit\n",
        "model = KerasRegressor(model=base_model, \n",
        "                       #here we put out our training args\n",
        "                       epochs=args.epochs,\n",
        "                       batch_size = args.batch_size,\n",
        "                       callbacks=[args.callback],\n",
        "\n",
        "                       #The KerasRegressor wrapper for sklearn uses the fit__ prefix to denote fitting args\n",
        "                       fit__validation_split = 0.25) \n",
        "\n",
        "#Now we fit our model and save the history for plotting later\n",
        "model.fit(trainx, trainy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7hoVbRo2N-9",
        "outputId": "33ebf3f5-9d88-4403-9117-12db378b281f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 18)                180       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 18)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 9)                 171       \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 9)                 0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 40        \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 396\n",
            "Trainable params: 396\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/500\n",
            "1407/1407 [==============================] - 13s 7ms/step - loss: 0.4155 - val_loss: 0.2786\n",
            "Epoch 2/500\n",
            "1407/1407 [==============================] - 10s 7ms/step - loss: 0.3412 - val_loss: 0.2832\n",
            "Epoch 3/500\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.3293 - val_loss: 0.2750\n",
            "Epoch 4/500\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.3221 - val_loss: 0.2659\n",
            "Epoch 5/500\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.3185 - val_loss: 0.2713\n",
            "Epoch 6/500\n",
            "1407/1407 [==============================] - 5s 4ms/step - loss: 0.3135 - val_loss: 0.2613\n",
            "Epoch 7/500\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.3143 - val_loss: 0.2687\n",
            "Epoch 8/500\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.3098 - val_loss: 0.2540\n",
            "Epoch 9/500\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.3089 - val_loss: 0.2618\n",
            "Epoch 10/500\n",
            "1407/1407 [==============================] - 5s 4ms/step - loss: 0.3054 - val_loss: 0.2613\n",
            "Epoch 11/500\n",
            "1407/1407 [==============================] - 5s 4ms/step - loss: 0.3045 - val_loss: 0.2695\n",
            "Epoch 12/500\n",
            "1407/1407 [==============================] - 5s 4ms/step - loss: 0.3040 - val_loss: 0.2617\n",
            "Epoch 13/500\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.3044 - val_loss: 0.2640\n",
            "Epoch 14/500\n",
            "1407/1407 [==============================] - 5s 4ms/step - loss: 0.3011 - val_loss: 0.2512\n",
            "Epoch 15/500\n",
            "1407/1407 [==============================] - 5s 4ms/step - loss: 0.3047 - val_loss: 0.2714\n",
            "Epoch 16/500\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.3032 - val_loss: 0.2740\n",
            "Epoch 17/500\n",
            "1407/1407 [==============================] - 5s 4ms/step - loss: 0.3036 - val_loss: 0.2706\n",
            "Epoch 18/500\n",
            "1407/1407 [==============================] - 6s 4ms/step - loss: 0.2997 - val_loss: 0.2663\n",
            "Epoch 19/500\n",
            "1407/1407 [==============================] - 5s 4ms/step - loss: 0.2996 - val_loss: 0.2632\n",
            "Epoch 20/500\n",
            "  89/1407 [>.............................] - ETA: 3s - loss: 0.3055"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Performance Evaluation"
      ],
      "metadata": {
        "id": "l5p7ANXb3FY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In order to monitor our performance and make an estimation, we need to do predictions on our sets\n",
        "traindf[\"y_pred\"] = decoder(model.predict(trainx).flatten())\n",
        "testdf[\"y_pred\"] = decoder(model.predict(testx).flatten())"
      ],
      "metadata": {
        "id": "ly4VR2NS3Em5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No we are going to create our Direct Loss Estimation. Even though we did not create a time series model, we'll be bringing our time back in. Remember that the idea of DLE is that we have ground truth over a certain period, deploy the model, then look for a change or drift over time."
      ],
      "metadata": {
        "id": "LauJ-X3_R8Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we create our Direct Loss Estimation Object\n",
        "estimator = nml.DLE(\n",
        "    #What columns are we using\n",
        "    feature_column_names=['car_age', 'km_driven', 'price_new', 'accident_count',\n",
        "                          'door_count', 'fuel', 'transmission'],\n",
        "    #What is the name of the prediction and truth columns\n",
        "    y_pred='y_pred',\n",
        "    y_true='y_true',\n",
        "    #Where is our timestamp\n",
        "    timestamp_column_name='timestamp',\n",
        "    #We will be using mean absolute error\n",
        "    metrics=['mae'],\n",
        "    #How many observations will be used to aggregate and compute the mean\n",
        "    chunk_size=6000,\n",
        ")\n",
        "\n",
        "#Now we fit to our training data\n",
        "estimator.fit(traindf)\n",
        "\n",
        "#and make our estimation on the test data, then convert that to a dataframe\n",
        "est_perf = estimator.estimate(testdf)\n",
        "est_perf_data = est_perf.to_df()"
      ],
      "metadata": {
        "id": "tOdv8Jrk5Vyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we can plot what our metric looks like\n",
        "fig = est_perf.filter(metrics=['mae']).plot()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "vWqZDlYS5kT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see above, our DLE estimator has noticed a big change in the Mean Absolute Error of our model. We think that the Mean Absolute Error drops suddenly after the model has deployed. Lets take a look at the actual test y values and see if we are right. "
      ],
      "metadata": {
        "id": "GvyRAoaqTg1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "xC27khNvV9Tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# add ground truth to analysis\n",
        "analysis_full = pd.concat([testdf, test_targets], axis = 1)\n",
        "df_all = pd.concat([traindf, analysis_full]).reset_index(drop=True)\n",
        "df_all['timestamp'] = pd.to_datetime(df_all['timestamp'])\n",
        "# calculate actual MAE\n",
        "target_col = estimator.y_true\n",
        "pred_score_col = 'y_pred'\n",
        "actual_performance = []\n",
        "for idx in est_perf_data.index:\n",
        "    start_date, end_date = est_perf_data.loc[idx, ('chunk', 'start_date')], est_perf_data.loc[idx, ('chunk', 'end_date')]\n",
        "    sub = df_all[df_all['timestamp'].between(start_date, end_date)]\n",
        "    actual_perf = mean_absolute_error(sub[target_col], sub[pred_score_col])\n",
        "    est_perf_data.loc[idx, ('mae', 'realized')] = actual_perf\n",
        "# plot\n",
        "first_analysis = est_perf_data[('chunk', 'start_date')].values[10]\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(est_perf_data[('chunk', 'start_date')], est_perf_data[('mae', 'value')], label='estimated MAE')\n",
        "plt.plot(est_perf_data[('chunk', 'start_date')], est_perf_data[('mae', 'realized')], label='actual MAE')\n",
        "plt.xticks(rotation=90)\n",
        "plt.axvline(x=first_analysis, label='First analysis chunk', linestyle=':', color='grey')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W74PW28e8gQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like our DLE did a really good job of estimating the Loss! Going forward we can use this to see if our model drifts away. No ground truth required!"
      ],
      "metadata": {
        "id": "ePBG8fnWUFPr"
      }
    }
  ]
}